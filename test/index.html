<!doctype html>
<html>
    <head>
        <title>Transformers.js V3 (WebGPU support)</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Search Bar Example</title>
        <style>
            .search-bar {
            width: 300px;
            position: relative;
            }
            .search-input {
            width: 100%;
            padding: 10px 40px 10px 10px;
            font-size: 16px;
            border: 1px solid #ddd;
            border-radius: 5px;
            }
            .search-button {
            position: absolute;
            right: 10px;
            top: 50%;
            transform: translateY(-50%);
            background: none;
            border: none;
            cursor: pointer;
            }
        </style>
    </head>
    <body>
        <script type="module">
            import { AutoModelForCausalLM, AutoTokenizer, pipeline, env, TextGenerationPipeline } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers';
            
            const pipeConfig = {
                dtype: 'q4f16',
                progress_callback: (p) => {
                    const state = document.getElementById('state');
                    if (p.status === "initiate"){
                        state.textContent = "Initializing...";
                    }else if (p.status === "progress"){
                        state.textContent = "Downloading model: " + Math.floor(Number(p.progress)) + "%";
                    }else if (p.status === "ready"){
                        state.textContent = "Model ready!";
                    }
                }
            }
            
            try {
                env.localModelPath = './models';
                env.allowLocalModels = true;
                env.allowRemoteModels = false; 

                const state = document.getElementById('device');
                state.textContent = "Device: " + (navigator.gpu ? "WebGPU" : "CPU");

                if(navigator.gpu){
                    pipeConfig.device = 'webgpu';
                }

                const tokenizer = await AutoTokenizer.from_pretrained("Llama-3.2-1B-Instruct");
                const generator = await pipeline("text-generation", "Llama-3.2-1B-Instruct", pipeConfig);

                class TokenList {
                    constructor() {
                        this.element = document.getElementById('resultDisplay');
                    }

                    put(list) {
                        this.element.textContent = this.element.textContent + tokenizer.decode(list[0], {skip_special_tokens: true,})
                    }

                    end() {
                        this.element = null;
                    }
                }
                const tokenlist = new TokenList();
                window.search = function search() {
                    const query = document.querySelector('.search-input').value;
                    (async () => {
                        const output = await generator(query, { 
                            max_new_tokens: 1024,
                            streamer: tokenlist,
                        });
                    })();
                }
            } catch (error) {
                console.log(error);
            }
        </script>
        <div class="search-bar">
            <input type="text" class="search-input" placeholder="Search...">
            <button class="search-button" onclick="search()">
              üîç
            </button>
        </div>
        <div class="result" id="state"></div>
        <div class="result" id="device"></div>
        <div class="result" id="resultDisplay"></div>

    </body>
</html>